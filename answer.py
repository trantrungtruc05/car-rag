"""
RAG Chatbot t∆∞ v·∫•n b√°n xe h∆°i
- T√¨m ki·∫øm xe t·ª´ database Qdrant
- Tr·∫£ l·ªùi c√¢u h·ªèi c√≥ ng·ªØ c·∫£nh v·ªõi LangChain
- Nh·ªõ l·ªãch s·ª≠ h·ªôi tho·∫°i
"""

import os
from qdrant_client import QdrantClient
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Qdrant
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# =============================================================================
# CONFIGURATION - C·∫•u h√¨nh
# =============================================================================

QDRANT_URL = "http://47.129.184.129:6333/"
COLLECTION_NAME = "car_sales_data"
EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-5.1-2025-11-13"
LLM_TEMPERATURE = 0.3
TOP_K_RESULTS = 7

# =============================================================================
# SETUP - Kh·ªüi t·∫°o components
# =============================================================================

def setup_vector_db():
    """Kh·ªüi t·∫°o Qdrant vector database v√† retriever"""
    qdrant_client = QdrantClient(url=QDRANT_URL)
    qdrant_client.get_collection(collection_name=COLLECTION_NAME)
    
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
    vector_db = Qdrant(
        client=qdrant_client,
        collection_name=COLLECTION_NAME,
        embeddings=embeddings
    )
    
    return vector_db.as_retriever(search_kwargs={"k": TOP_K_RESULTS})

def setup_llm():
    """Kh·ªüi t·∫°o ChatGPT model"""
    return ChatOpenAI(model_name=LLM_MODEL, temperature=LLM_TEMPERATURE)

# Kh·ªüi t·∫°o
retriever = setup_vector_db()
llm = setup_llm()

# =============================================================================
# PROMPTS - Template c√¢u h·ªèi
# =============================================================================

# Prompt 1: Vi·∫øt l·∫°i c√¢u h·ªèi th√†nh c√¢u ƒë·ªôc l·∫≠p c√≥ ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh
contextualize_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "Nhi·ªám v·ª•: Vi·∫øt l·∫°i c√¢u h·ªèi cu·ªëi c·ªßa user th√†nh m·ªôt c√¢u h·ªèi ƒê·ªòC L·∫¨P c√≥ ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh.\n\n"
     "QUY T·∫ÆC QUAN TR·ªåNG:\n"
     "1. CH·ªà vi·∫øt l·∫°i c√¢u h·ªèi, KH√îNG ƒë∆∞·ª£c tr·∫£ l·ªùi c√¢u h·ªèi\n"
     "2. N·∫øu c√¢u h·ªèi ƒë√£ ƒë·ªôc l·∫≠p, gi·ªØ nguy√™n\n"
     "3. N·∫øu c√¢u h·ªèi thi·∫øu ng·ªØ c·∫£nh, th√™m th√¥ng tin t·ª´ l·ªãch s·ª≠ chat\n\n"
     "V√≠ d·ª•:\n"
     "- Input: 'Xe n√†o r·∫ª nh·∫•t?' (sau khi h·ªèi v·ªÅ Toyota)\n"
     "- Output: 'Trong c√°c xe Toyota v·ª´a ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn, xe n√†o c√≥ gi√° r·∫ª nh·∫•t?'\n"
     "- KH√îNG ƒê∆Ø·ª¢C: 'Toyota Vios l√† xe r·∫ª nh·∫•t...' (ƒë√¢y l√† tr·∫£ l·ªùi, kh√¥ng ph·∫£i vi·∫øt l·∫°i!)"),
    MessagesPlaceholder("chat_history"),
    ("human", "Vi·∫øt l·∫°i c√¢u h·ªèi sau th√†nh c√¢u ƒë·ªôc l·∫≠p: {question}")
])

# Prompt 2: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n context t·ª´ database
answer_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "B·∫°n l√† chuy√™n gia t∆∞ v·∫•n b√°n xe h∆°i th√¥ng th√°i.\n"
     "Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n CONTEXT (kho xe). N·∫øu kh√¥ng c√≥ xe ph√π h·ª£p, n√≥i r√µ l√† kho ch∆∞a c√≥.\n"
     "Tr·∫£ l·ªùi ng·∫Øn g·ªçn, li·ªát k√™ d·∫°ng bullet n·∫øu l√† danh s√°ch."),
    MessagesPlaceholder("chat_history"),
    ("human",
     "CONTEXT:\n{context}\n\n"
     "C√¢u h·ªèi: {question}")
])

# =============================================================================
# CHAINS - Chu·ªói x·ª≠ l√Ω
# =============================================================================

# Chain 1: Vi·∫øt l·∫°i c√¢u h·ªèi c√≥ ng·ªØ c·∫£nh
standalone_question_chain = (
    contextualize_prompt
    | llm
    | StrOutputParser()
)

def retrieve_context(inputs: dict) -> str:
    """
    L·∫•y context t·ª´ vector database
    1. Vi·∫øt l·∫°i c√¢u h·ªèi th√†nh c√¢u ƒë·ªôc l·∫≠p
    2. T√¨m ki·∫øm trong Qdrant
    3. Tr·∫£ v·ªÅ context d·∫°ng text
    """
    standalone_q = standalone_question_chain.invoke({
        "question": inputs["question"],
        "chat_history": inputs.get("chat_history", [])
    })
    docs = retriever.invoke(standalone_q)
    return "\n\n".join([d.page_content for d in docs])

# Chain 2: RAG pipeline ho√†n ch·ªânh (ch∆∞a c√≥ memory)
base_rag_chain = (
    {
        "context": retrieve_context,  # L·∫•y context t·ª´ database
        "question": lambda x: x["question"],  # L·∫•y c√¢u h·ªèi g·ªëc
        "chat_history": lambda x: x.get("chat_history", []),  # L·∫•y l·ªãch s·ª≠
    }
    | answer_prompt  # Gh√©p v√†o prompt
    | llm  # G·ª≠i cho ChatGPT
    | StrOutputParser()  # Parse output
)

# =============================================================================
# MEMORY - Qu·∫£n l√Ω l·ªãch s·ª≠ h·ªôi tho·∫°i
# =============================================================================

_store = {}  # Dictionary l∆∞u l·ªãch s·ª≠: session_id -> ChatMessageHistory

def get_session_history(session_id: str) -> ChatMessageHistory:
    """L·∫•y ho·∫∑c t·∫°o m·ªõi l·ªãch s·ª≠ chat cho session"""
    if session_id not in _store:
        _store[session_id] = ChatMessageHistory()
    return _store[session_id]

# Chain 3: RAG v·ªõi memory - T·ª∞ ƒê·ªòNG l∆∞u/load l·ªãch s·ª≠
rag_chain_with_memory = RunnableWithMessageHistory(
    base_rag_chain,
    get_session_history,
    input_messages_key="question",
    history_messages_key="chat_history",
)

# =============================================================================
# DEMO - Ch·∫°y th·ª≠
# =============================================================================

if __name__ == "__main__":
    session_id = "truc_car_chat_001"
    
    # C√¢u h·ªèi 1
    print("-" * 50)
    print("üîç C√¢u h·ªèi 1: Cho t√¥i danh s√°ch t·∫ßm 5 xe √¥ t√¥ honda ?")
    print("-" * 50)
    response1 = rag_chain_with_memory.invoke(
        {"question": "Cho t√¥i danh s√°ch t·∫ßm 5 xe √¥ t√¥ honda ?"},
        config={"configurable": {"session_id": session_id}},
    )
    print(response1)
    
    # C√¢u h·ªèi 2 - Bot s·∫Ω nh·ªõ c√¢u 1
    print("\n" + "-" * 50)
    print("üîç C√¢u h·ªèi 2: Trong danh s√°ch b·∫°n m·ªõi g·ª≠i, xe n√†o m·∫Øc nh·∫•t")
    print("-" * 50)
    response2 = rag_chain_with_memory.invoke(
        {"question": "Trong danh s√°ch b·∫°n m·ªõi g·ª≠i, xe n√†o m·∫Øc nh·∫•t"},
        config={"configurable": {"session_id": session_id}},
    )
    print(response2)


    # C√¢u h·ªèi 3 - Bot s·∫Ω nh·ªõ c√¢u 1 v√† c√¢u 2
    print("\n" + "-" * 50)
    print("üîç C√¢u h·ªèi 3: Cho t√¥i m√¥ t·∫£ xe n√†y v·ªõi")
    print("-" * 50)
    response3 = rag_chain_with_memory.invoke(
        {"question": "Cho t√¥i m√¥ t·∫£ xe n√†y v·ªõi"},
        config={"configurable": {"session_id": session_id}},
    )
    print(response3)